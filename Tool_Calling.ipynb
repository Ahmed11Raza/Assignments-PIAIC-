{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4bfQMLb1YVZnlGp6FZHI0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmed11Raza/Assignments-PIAIC-/blob/main/Tool_Calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gemini API: Tool/Function calling with Python:"
      ],
      "metadata": {
        "id": "h71DzQYuZ3Hv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step by step Guide:-"
      ],
      "metadata": {
        "id": "bRZvq0I3aET7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Function Description:**\n",
        "You first define a function (or multiple functions) within your code. Each function serves a specific task, and you can provide a description of what each function does. For example, you could define a function for adding two numbers or fetching the current weather.\n",
        "\n",
        "**2. Request with Description:**\n",
        "When making a request to an AI model, you can include a description of what you want the AI to perform. The request includes not just the input data but also a detailed description of the task or the function you want to call.\n",
        "\n",
        "**3. AI Identifies Matching Function:**\n",
        "The AI model processes the request and matches the description to one of the defined functions. This is possible because the model has been trained to recognize patterns and match descriptions with appropriate functions.\n",
        "\n",
        "**4. Function Name and Arguments:**\n",
        "Once the model identifies the correct function, it will return the name of that function along with any required arguments. The returned function can then be called programmatically with those arguments to perform the task.\n",
        "\n",
        "**5. Real-World Example:**\n",
        "Imagine you're building a chatbot that helps users check the weather. You define a function called get_weather that takes a location as an argument and returns weather data. When the user asks the chatbot for the weather in a city, the model will:\n",
        "\n",
        "Identify that the \"*get_weather*\" function matches the description of what the user is asking for.\n",
        "Provide the function name (*get_weather*) along with the location as the argument."
      ],
      "metadata": {
        "id": "RGPHtvh1a03M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies:"
      ],
      "metadata": {
        "id": "cV6cUw8mbdy0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "byIjZ-cYZ1uX"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.7.2\"  # Install the Python SDK"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "62SCPQbKbsRg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up your API key:\n",
        "## To run the following cell, your API key must be stored it in a Colab Secret named GOOGLE_API_KEY. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the Authentication quickstart for an example."
      ],
      "metadata": {
        "id": "jEY92A6Lbzxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "rQg2dTG9cLwu"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function calling basics"
      ],
      "metadata": {
        "id": "ilBups9EcWF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function calling in AI models allows you to define functions in your code and let the model choose and call the correct one based on a user’s request.\n",
        "\n",
        "**Here’s how it works:**\n",
        "\n",
        "Define Functions: You create functions with clear names, descriptions, and expected input types (like int, float, str, list, etc.).\n",
        "Pass Functions to the Model: These functions are given to the model as \"tools,\" which it can use when responding to requests.\n",
        "Model Chooses the Right Function: Based on the user's input, the model looks at the function descriptions and parameter types to pick the correct function and provide the required arguments.\n",
        "Function Call: The model sends back the function name and arguments, which your code then uses to call the function and get the result.\n",
        "Allowed Parameter Types:\n",
        "int, float, bool, str, list, dict (only these types are supported).\n",
        "**Example**:\n",
        "\n",
        "You define a function to add two numbers, and when a user asks for a sum, the model calls your function with the numbers."
      ],
      "metadata": {
        "id": "67tJp2_vcbFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add(a: float, b: float):\n",
        "    \"\"\"returns a + b.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: float, b: float):\n",
        "    \"\"\"returns a - b.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "def multiply(a: float, b: float):\n",
        "    \"\"\"returns a * b.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def divide(a: float, b: float):\n",
        "    \"\"\"returns a / b.\"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\", tools=[add, subtract, multiply, divide]\n",
        ")\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DD9XMU9dEMv",
        "outputId": "ee942a96-9601-47a8-d8a7-904ff97bc943"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-1.5-flash',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=<google.generativeai.types.content_types.FunctionLibrary object at 0x782432c601c0>,\n",
              "    system_instruction=None,\n",
              "    cached_content=None\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Automatic function calling:"
      ],
      "metadata": {
        "id": "o0UfZeX_dVj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function calls in multi-turn chats are designed to handle interactive, back-and-forth communication between the user and the AI model. They are especially useful when the AI needs to dynamically select and execute functions based on the evolving context of a conversation.\n",
        "\n",
        "# Key Points:\n",
        "**Natural Fit for Multi-Turn Chats:**\n",
        "\n",
        "In a multi-turn chat, the user's request might involve multiple steps or require clarification (e.g., \"Get the weather for Karachi,\" followed by \"What about tomorrow?\").\n",
        "Function calls enable the model to respond appropriately by selecting and executing functions in real-time based on the ongoing conversation.\n",
        "Using Python SDK's ChatSession:\n",
        "\n",
        "The ChatSession class in the Python SDK manages the conversation history, so you don’t need to manually track past interactions.\n",
        "The session ensures that the model has context from previous turns, allowing it to provide coherent and context-aware responses.\n",
        "enable_automatic_function_calling:\n",
        "\n",
        "By enabling the enable_automatic_function_calling parameter in the chat session, the SDK simplifies the process of function calling.\n",
        "With this parameter, the model can:\n",
        "Automatically determine which function to call based on the user's input.\n",
        "Provide the necessary arguments for the function.\n",
        "Integrate the function output into its response without requiring extra steps from the developer.\n",
        "**Example Workflow:**\n",
        "1. User Request:\n",
        "\n",
        "User: \"Get the weather for Karachi.\"\n",
        "2. AI Response:\n",
        "\n",
        "The model identifies a function (e.g., get_weather) and provides the argument (location = \"Karachi\").\n",
        "3. Function Call:\n",
        "\n",
        "The SDK automatically handles calling the get_weather function with the correct argument.\n",
        "4. Result Integration:\n",
        "\n",
        "The model receives the function's output and includes it in its response.\n",
        "Model: \"The weather in Karachi is sunny.\"\n",
        "5. Next Turn:\n",
        "\n",
        " User: \"What about tomorrow?\"\n",
        "The session maintains context, so the model knows the user is still asking about Karachi’s weather and can handle this seamlessly.\n",
        "**Benefits:**\n",
        "\n",
        "1) Simplified Development: You don’t need to manually track conversation history or map inputs to functions.\n",
        "\n",
        "2) Dynamic and Interactive: Functions are selected and called dynamically based on the context of the conversation.\n",
        "\n",
        "3) Coherent Conversations: The session maintains continuity, so users can have natural, multi-turn interactions."
      ],
      "metadata": {
        "id": "KpbFLtHrdY-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = model.start_chat(enable_automatic_function_calling=True)"
      ],
      "metadata": {
        "id": "mNiIEATxdYF6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# When automatic function calling is enabled in the Python SDK's ChatSession, the send_message method does more than just generate text responses—it can also automatically identify and execute your predefined functions if the model determines one is needed to answer the user's query.\n",
        "\n",
        "# This process integrates function calling seamlessly into the chat flow, making it appear as though the model is directly responding with the correct answer, even though it may have called a function behind the scenes to compute or retrieve the result."
      ],
      "metadata": {
        "id": "JpEp0trGfk5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How It Works:\n",
        "\n",
        "**1) Automatic Function Execution:**\n",
        "\n",
        "When the user sends a message *(e.g., \"What’s the weather in Karachi?\")*, the send_message method:\n",
        "Processes the user input.\n",
        "      Determines if a function from the provided tools should be called based on the context and user intent.\n",
        "If a function is needed, the SDK automatically executes it with the correct arguments.\n",
        "\n",
        "**2) Combining Results into a Response:**\n",
        "\n",
        "After the function is called, the SDK integrates the result into the AI's response as if it were part of a standard text reply.\n",
        "\n",
        "**3) Simplified Development:**\n",
        "\n",
        "You don’t need to write extra code to handle the function call manually—the SDK manages everything, from selecting the function to including the result in the reply."
      ],
      "metadata": {
        "id": "_tQtT5RnfsB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\n",
        "    \"I have 25 dogs, each has 4 legs, how many legs are there in total?\"\n",
        ")\n",
        "response.text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IQK_92yJgaBo",
        "outputId": "e8995780-ace8-4bcf-d0f4-8453c9d9eece"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are 100 legs in total.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ChatSession.history property in the Python SDK allows you to examine the entire flow of a conversation, showing how function calls are seamlessly integrated into the interaction between the user and the model.\n",
        "\n",
        "# Here’s a breakdown of how it works:\n",
        "**What ChatSession.history Tracks:**\n",
        "\n",
        "**Conversation Turns:**\n",
        "Each user input or model response is logged as a turn in the conversation.\n",
        "\n",
        "**Content Details:** Each turn is represented by a genai.protos.Content object, which contains:\n",
        "Role: Indicates who initiated the content:\n",
        "\"user\": Messages sent by the user.\n",
        "\"model\": Responses generated by the AI model.\n",
        "Parts: The components of the message, which could include:\n",
        "Text: Plain text messages.\n",
        "\n",
        "**Function Call:**\n",
        " The model's request to execute a specific function.\n",
        "Function Response: The result returned after the requested function is executed.\n",
        "\n",
        "*Example:*\n",
        " Mittens Calculation\n",
        "Let’s say the user asks a question that requires a function call to compute the result.\n",
        "\n",
        "# Conversation Flow:\n",
        "**User Input:**\n",
        "\n",
        "1) The user asks: \"I have 57 cats, each owns 44 mittens, how many mittens is that in total?\"\n",
        "This is logged in the conversation history as a \"user\" turn with plain text.\n",
        "\n",
        "2)Model Function Call:\n",
        "\n",
        "The model determines it needs a multiplication function to calculate the total mittens.\n",
        "It logs a \"model\" turn with a FunctionCall part, specifying:\n",
        "Function: multiply\n",
        "Arguments: { \"a\": 57, \"b\": 44 }\n",
        "\n",
        "3)Automatic Function Execution:\n",
        "\n",
        "Since enable_automatic_function_calling is enabled, the ChatSession automatically executes the multiply function and calculates the result (57 × 44 = 2508).\n",
        "A \"user\" turn with a FunctionResponse part is added to the history, showing the returned value: 2508.\n",
        "\n",
        "**Model Response**:\n",
        "\n",
        "The model uses the function result to generate its reply: *\"The total number of mittens is 2508.\"*\n",
        "This is logged as a \"model\" turn with plain text.\n",
        "\n",
        "**Benefits of ChatSession.history:**\n",
        "\n",
        "Full Transparency: You can see how each interaction and function call contributes to the final response.\n",
        "\n",
        "**Debugging Made Easy:**\n",
        " By examining the sequence of turns, you can troubleshoot issues like incorrect function arguments or responses.\n",
        "Traceable Conversations: The flow is stored chronologically, providing a clear timeline of the interaction."
      ],
      "metadata": {
        "id": "GhljLbIeg18l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idp90Mggi7PA",
        "outputId": "9067cb98-119f-421c-a131-71cafa15fb80"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> [{'text': 'I have 25 dogs, each has 4 legs, how many legs are there in total?'}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'multiply', 'args': {'a': 25.0, 'b': 4.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'multiply', 'response': {'result': 100.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'text': 'There are 100 legs in total.\\n'}]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [part.to_dict() if hasattr(part, 'to_dict') else str(part) for part in content.parts])\n",
        "    print(\"=\" * 80)  # Changed divider style\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRkIguUcjKPr",
        "outputId": "27933d92-65aa-4a25-cb91-e1c07ba84e04"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user -> ['text: \"I have 25 dogs, each has 4 legs, how many legs are there in total?\"\\n']\n",
            "================================================================================\n",
            "model -> ['function_call {\\n  name: \"multiply\"\\n  args {\\n    fields {\\n      key: \"b\"\\n      value {\\n        number_value: 4\\n      }\\n    }\\n    fields {\\n      key: \"a\"\\n      value {\\n        number_value: 25\\n      }\\n    }\\n  }\\n}\\n']\n",
            "================================================================================\n",
            "user -> ['function_response {\\n  name: \"multiply\"\\n  response {\\n    fields {\\n      key: \"result\"\\n      value {\\n        number_value: 100\\n      }\\n    }\\n  }\\n}\\n']\n",
            "================================================================================\n",
            "model -> ['text: \"There are 100 legs in total.\\\\n\"\\n']\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual function calling"
      ],
      "metadata": {
        "id": "b-yWl8UBk_JX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more control, you can process [`genai.protos.FunctionCall`](https://ai.google.dev/api/python/google/generativeai/protos/FunctionCall) requests from the model yourself. This would be the case if:\n",
        "\n",
        "- You use a `ChatSession` with the default `enable_automatic_function_calling=False`.\n",
        "- You use `GenerativeModel.generate_content` (and manage the chat history yourself).\n",
        "\n",
        "The following example is a rough equivalent of the [function calling single-turn curl sample](https://ai.google.dev/docs/function_calling#function-calling-single-turn-curl-sample) in Python. It uses functions that return (mock) movie playtime information, possibly from a hypothetical API:"
      ],
      "metadata": {
        "id": "SitodhXblBRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_cuisines(description: str, location: str = \"\"):\n",
        "    \"\"\"Find cuisines or dishes available in restaurants based on description, type, or popular keywords.\n",
        "\n",
        "    Args:\n",
        "        description: Any kind of description including cuisine type, popular dishes, or dietary preferences.\n",
        "        location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "    \"\"\"\n",
        "    return [\"Italian\", \"Mexican\"]\n",
        "\n",
        "\n",
        "def find_restaurants(location: str, cuisine: str = \"\"):\n",
        "    \"\"\"Find restaurants based on location and optionally filter by cuisine.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "        cuisine: Type of cuisine to filter restaurants by\n",
        "    \"\"\"\n",
        "    return [\"Pasta Palace\", \"Taco Tower\"]\n",
        "\n",
        "\n",
        "def get_reservation_times(location: str, restaurant: str, cuisine: str, date: str):\n",
        "    \"\"\"\n",
        "    Find available reservation times for a specific restaurant.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "        restaurant: Name of the restaurant\n",
        "        cuisine: Type of cuisine served at the restaurant\n",
        "        date: Date for requested reservation\n",
        "    \"\"\"\n",
        "    return [\"7:00 PM\", \"8:30 PM\"]\n"
      ],
      "metadata": {
        "id": "ZWnxuGlglPEy"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "\n",
        "**Function 1:**\n",
        " **find_cuisines**\n",
        "\n",
        "**Purpose:** Lists cuisines or dishes based on description or preferences.\n",
        "\n",
        "**Arguments:**\n",
        "**description:**\n",
        " Keywords like \"spicy food\" or \"vegan options\".\n",
        "\n",
        "**location:** Optionally specify a location to narrow results."
      ],
      "metadata": {
        "id": "Op1PdgDBltSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a dictionary to make looking up functions by name easier later on. You can also use it to pass the array of functions to the `tools` parameter of `GenerativeModel`."
      ],
      "metadata": {
        "id": "PSy5iGk6mRyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "functions = {\n",
        "    \"find_cuisines\": find_cuisines,\n",
        "    \"find_restaurants\": find_restaurants,\n",
        "    \"get_reservation_times\": get_reservation_times,\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=functions.values())\n"
      ],
      "metadata": {
        "id": "CwQCYB74mTEF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After using `generate_content()` to ask a question, the model requests a `function_call`:"
      ],
      "metadata": {
        "id": "RkXW_u-nmpHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\n",
        "    \"Which restaurants in San Francisco serve Italian cuisine?\"\n",
        ")\n",
        "response.candidates[0].content.parts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "QCqOalS5myqo",
        "outputId": "4493d248-f1a5-4e71-b111-946fab8f6895"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[function_call {\n",
              "  name: \"find_restaurants\"\n",
              "  args {\n",
              "    fields {\n",
              "      key: \"location\"\n",
              "      value {\n",
              "        string_value: \"San Francisco\"\n",
              "      }\n",
              "    }\n",
              "    fields {\n",
              "      key: \"cuisine\"\n",
              "      value {\n",
              "        string_value: \"Italian\"\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of manually using if statements to call specific functions (e.g., find_restaurants, find_cuisines, etc.), you can leverage the functions dictionary to simplify the process. The dictionary allows you to dynamically call the appropriate function by its name, making the code cleaner and more maintainable.\n"
      ],
      "metadata": {
        "id": "iudjDFtanE4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is not using a `ChatSession` with automatic function calling, you have to call the function yourself.\n",
        "\n",
        "A very simple way to do this would be with `if` statements:\n",
        "\n",
        "```python\n",
        "if function_call.name == 'find_restaurants':\n",
        "  find_restaurants(**function_call.args)\n",
        "elif ...\n",
        "```\n",
        "\n",
        "However, since you already made the `functions` dictionary, this can be simplified to:"
      ],
      "metadata": {
        "id": "7ngB4cD0nkDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_function(function_call, functions):\n",
        "    function_name = function_call.name\n",
        "    function_args = function_call.args\n",
        "    return functions[function_name](**function_args)\n",
        "\n",
        "\n",
        "part = response.candidates[0].content.parts[0]\n",
        "\n",
        "# Check if it's a function call; in real use you'd need to also handle text\n",
        "# responses as you won't know what the model will respond with.\n",
        "if part.function_call:\n",
        "    result = call_function(part.function_call, functions)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkfptfSOnZEA",
        "outputId": "d778156b-b3fa-47dc-ca7c-27b17d559e5d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pasta Palace', 'Taco Tower']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The final step involves continuing the conversation by passing the function response along with the message history to the model's generate_content() method. This allows the model to generate a cohesive and context-aware reply based on the earlier interaction."
      ],
      "metadata": {
        "id": "4c-7U1W0oU77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.protobuf.struct_pb2 import Struct\n",
        "\n",
        "# Simulated function result (ensure this matches the model's expectations)\n",
        "result = {\"restaurants\": [\"Pasta Palace\", \"Taco Tower\"]}\n",
        "\n",
        "# Put the result in a protobuf Struct\n",
        "s = Struct()\n",
        "s.update(result)\n",
        "\n",
        "# Correct function response to match the function call\n",
        "function_response = genai.protos.Part(\n",
        "    function_response=genai.protos.FunctionResponse(name=\"find_restaurants\", response=s)\n",
        ")\n",
        "\n",
        "# Build the message history\n",
        "# Ensure the number of function call parts and function response parts is equal\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"parts\": [\"Which restaurants in San Francisco serve Italian cuisine?\"]},\n",
        "    {\"role\": \"model\", \"parts\": response.candidates[0].content.parts},  # Ensure this part aligns with the model's response\n",
        "    {\"role\": \"user\", \"parts\": [function_response]},  # Correctly structured function response\n",
        "]\n",
        "\n",
        "# Generate the next response\n",
        "response = model.generate_content(messages)\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "zVzWZqT7pKZJ",
        "outputId": "4cf49e01-0279-4409-fd21-7440bae0df86"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the available information, Pasta Palace and Taco Tower are in San Francisco and serve Italian food.  However, it's important to note that the response may be incomplete or inaccurate as the available tools may not have comprehensive data.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function calling chain\n",
        "\n",
        "The model is not limited to one function call, it can chain them until it finds the right answer."
      ],
      "metadata": {
        "id": "dX5FhceqreSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the chat session with automatic function calling enabled\n",
        "chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "\n",
        "# Send a message asking about restaurants serving Italian cuisine in San Francisco\n",
        "response = chat.send_message(\n",
        "    \"Which restaurants in San Francisco serve Italian cuisine?\"\n",
        ")\n",
        "\n",
        "# Iterate through the chat history to print the conversation\n",
        "for content in chat.history:\n",
        "    print(f\"Role: {content.role} -> \", [type(part).to_dict(part) for part in content.parts])\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Optional: Handle the function response for restaurants\n",
        "for content in chat.history:\n",
        "    if content.role == \"model\" and hasattr(content, 'function_response'):\n",
        "        print(f\"Function Response: {content.function_response}\")\n",
        "        print(\"-\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "umhS2yUWrOQF",
        "outputId": "9ac395bf-bf4c-4043-c93b-d7e034878116"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Role: user ->  [{'text': 'Which restaurants in San Francisco serve Italian cuisine?'}]\n",
            "--------------------------------------------------------------------------------\n",
            "Role: model ->  [{'function_call': {'name': 'find_restaurants', 'args': {'location': 'San Francisco', 'cuisine': 'Italian'}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "Role: user ->  [{'function_response': {'name': 'find_restaurants', 'response': {'result': ['Pasta Palace', 'Taco Tower']}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "Role: model ->  [{'text': 'Pasta Palace and Taco Tower are two restaurants in San Francisco that serve Italian cuisine.  Note that this is based on the limited data available from the API.  There may be other Italian restaurants in San Francisco.\\n'}]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can see that the model made three calls to answer your question and used the outputs of them in the subsequent calls and in the final answer."
      ],
      "metadata": {
        "id": "-YWQj9LHsfne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel function calls\n",
        "\n",
        "The Gemini API can call multiple functions in a single turn. This caters for scenarios where there are multiple function calls that can take place independently to complete a task.\n",
        "\n",
        "First set the tools up. Unlike the Food Restuarant example above, these functions do not require input from each other to be called so they should be good candidates for parallel calling."
      ],
      "metadata": {
        "id": "cOS4iTOtskF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def power_disco_ball(power: bool) -> bool:\n",
        "    \"\"\"Powers the spinning disco ball.\"\"\"\n",
        "    print(f\"Disco ball is {'spinning!' if power else 'stopped.'}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def start_music(energetic: bool, loud: bool, bpm: int) -> str:\n",
        "    \"\"\"Play some music matching the specified parameters.\n",
        "\n",
        "    Args:\n",
        "      energetic: Whether the music is energetic or not.\n",
        "      loud: Whether the music is loud or not.\n",
        "      bpm: The beats per minute of the music.\n",
        "\n",
        "    Returns: The name of the song being played.\n",
        "    \"\"\"\n",
        "    print(f\"Starting music! {energetic=} {loud=}, {bpm=}\")\n",
        "    return \"Never gonna give you up.\"\n",
        "\n",
        "\n",
        "def dim_lights(brightness: float) -> bool:\n",
        "    \"\"\"Dim the lights.\n",
        "\n",
        "    Args:\n",
        "      brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
        "    \"\"\"\n",
        "    print(f\"Lights are now set to {brightness:.0%}\")\n",
        "    return True"
      ],
      "metadata": {
        "id": "cDOjrYI1s9RN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now call the model with an instruction that could use all of the specified tools."
      ],
      "metadata": {
        "id": "7i8q0n9StCMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model up with tools.\n",
        "house_fns = [power_disco_ball, start_music, dim_lights]\n",
        "# Try this out with Pro and Flash...\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", tools=house_fns)\n",
        "\n",
        "# Call the API.\n",
        "chat = model.start_chat()\n",
        "response = chat.send_message(\"Turn this place into a party!\")\n",
        "\n",
        "# Print out each of the function calls requested from this single call.\n",
        "for part in response.parts:\n",
        "    if fn := part.function_call:\n",
        "        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
        "        print(f\"{fn.name}({args})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "eB2I_RFNtC-x",
        "outputId": "6c54199b-1b90-4344-807f-4a7c619b1c09"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "power_disco_ball(power=True)\n",
            "start_music(loud=True, energetic=True, bpm=120.0)\n",
            "dim_lights(brightness=0.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested."
      ],
      "metadata": {
        "id": "wZbTusRLtYBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate the responses from the specified tools.\n",
        "responses = {\n",
        "    \"power_disco_ball\": True,\n",
        "    \"start_music\": \"Never gonna give you up.\",\n",
        "    \"dim_lights\": True,\n",
        "}\n",
        "\n",
        "# Build the response parts.\n",
        "response_parts = [\n",
        "    genai.protos.Part(function_response=genai.protos.FunctionResponse(name=fn, response={\"result\": val}))\n",
        "    for fn, val in responses.items()\n",
        "]\n",
        "\n",
        "response = chat.send_message(response_parts)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "qGOjkiFDtL55",
        "outputId": "67ed3b1d-c882-41d7-c2ba-a2904c641d5e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Party started!  Never gonna give you up is now playing. The disco ball is spinning and the lights are dimmed to 50% brightness.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "### Useful API references:\n",
        "\n",
        "- The [genai.GenerativeModel](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/GenerativeModel.md) class\n",
        "  - Its [GenerativeModel.generate_content](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/GenerativeModel.md#generate_content) method builds a [genai.protos.GenerateContentRequest](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/protos/GenerateContentRequest.md) behind the scenes.\n",
        "    - The request's `.tools` field contains a list of 1 [genai.protos.Tool](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/protos/Tool.md) object.\n",
        "    - The tool's `function_declarations` attribute contains a list of [FunctionDeclarations](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/protos/FunctionDeclaration.md) objects.\n",
        "- The [response](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/protos/GenerateContentResponse.md) may contain a [genai.protos.FunctionCall](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/protos/FunctionCall.md), in `response.candidates[0].contents.parts[0]`.\n",
        "- if `enable_automatic_function_calling` is set the [genai.ChatSession](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/ChatSession.md) executes the call, and sends back the [genai.protos.FunctionResponse](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/protos/FunctionResponse.md).\n",
        "- In response to a [FunctionCall](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/protos/FunctionCall.md) the model always expects a [FunctionResponse](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/protos/FunctionResponse.md).\n",
        "- If you reply manually using [chat.send_message](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/ChatSession.md#send_message) or [model.generate_content](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/GenerativeModel.md#generate_content) remember thart the API is stateless you have to send the whole conversation history (a list of [content](https://github.com/google-gemini/generative-ai-python/blob/main/docs/api/google/generativeai/protos/Content.md) objects), not just the last one containing the `FunctionResponse`.\n",
        "\n",
        "### Related examples\n",
        "\n",
        "Check those examples using function calling to give you more ideas on how to use that very useful feature:\n",
        "* [Barista Bot](../examples/Agents_Function_Calling_Barista_Bot.ipynb), an agent to order coffee\n",
        "* Using function calling to [re-rank seach results](../examples/Search_reranking_using_embeddings.ipynb)"
      ],
      "metadata": {
        "id": "cN6ThvUbuYW5"
      }
    }
  ]
}